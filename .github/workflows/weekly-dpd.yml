name: Weekly DPD scrape

on:
  schedule:
    - cron: "0 3 * * 3"   # Every Wednesday 03:00 UTC (edit as needed)
  workflow_dispatch: {}    # Manual trigger button

jobs:
  run-scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run weekly job (prod settings)
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          ARTIFACT_DIR: "artifacts"
          # --- scraper knobs (tune as you like) ---
          SCRAPER_TIMEOUT: "30"
          SCRAPER_RETRIES: "3"
          SCRAPER_RETRY_SLEEP: "1.0"
          SCRAPER_MAX_DEPTH: "1"
          SCRAPER_ENRICH_FLUSH_EVERY: "50"
          SCRAPER_REQUEST_SLEEP: "0.05"
          SCRAPER_TARGET_MIN_ROWS: "59000"   # full corpus target for prod
          SCRAPER_MAX_ROWS: "0"              # 0 = no cap
          SCRAPER_SWEEP_ORDER: "din-first"
          SCRAPER_SWEEP_MAX_EMPTY: "0"
          SCRAPER_SWEEP_ENDPOINT: "DISPATCH_URL:GET"
          SCRAPER_BASELINE: "DISPATCH_URL:GET"
        run: |
          python -m scripts.run_weekly_job

      - name: Upload artifacts (xlsx/json/csv)
        uses: actions/upload-artifact@v4
        with:
          name: dpd-snapshot
          path: artifacts/**/*
